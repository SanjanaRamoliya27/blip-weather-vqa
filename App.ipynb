{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+/BOVJL9SflJhdlxAaFk+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjanaRamoliya27/blip-weather-vqa/blob/main/App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio opencv-python pillow imagehash\n"
      ],
      "metadata": {
        "id": "zCegUzQr6PxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "\n",
        "def image_basic_info(image):\n",
        "    width, height = image.size\n",
        "    aspect_ratio = round(width / height, 2)\n",
        "    return width, height, aspect_ratio\n",
        "\n",
        "def blur_score(image):\n",
        "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
        "    score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "    return round(score, 2)\n",
        "\n",
        "def perceptual_hash(image):\n",
        "    return str(imagehash.phash(image))\n"
      ],
      "metadata": {
        "id": "RCJlFIZp6UVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "-5UeDJYE6WUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "\n",
        "from utils import image_basic_info, blur_score\n"
      ],
      "metadata": {
        "id": "u2o98gCo6Yft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_image_with_vqa(img, question):\n",
        "    if img is None or question.strip() == \"\":\n",
        "        return \"‚ö†Ô∏è Please upload an image and enter a question.\"\n",
        "\n",
        "    # -------- EDA --------\n",
        "    width, height, aspect_ratio = image_basic_info(img)\n",
        "    blur = blur_score(img)\n",
        "\n",
        "    quality = \"Good\"\n",
        "    if blur < 100:\n",
        "        quality = \"Low Quality (Blurry)\"\n",
        "\n",
        "    # -------- VQA --------\n",
        "    inputs = processor(img, question, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs)\n",
        "\n",
        "    answer = processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    result = f\"\"\"\n",
        "üñºÔ∏è IMAGE EDA\n",
        "-------------------------\n",
        "Width           : {width}\n",
        "Height          : {height}\n",
        "Aspect Ratio    : {aspect_ratio}\n",
        "Blur Score      : {blur}\n",
        "Image Quality   : {quality}\n",
        "\n",
        "ü§ñ VISUAL QUESTION ANSWERING\n",
        "-------------------------\n",
        "Question : {question}\n",
        "Answer   : {answer}\n",
        "\"\"\"\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "nurRK6716asQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(title=\"Image EDA & Visual Question Answering (VQA)\") as demo:\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # üñºÔ∏è Image EDA & Visual Question Answering (VQA)\n",
        "        Analyze image quality and ask questions using the BLIP VQA model.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "\n",
        "        # ---------- LEFT COLUMN ----------\n",
        "        with gr.Column(scale=1):\n",
        "            image_input = gr.Image(\n",
        "                type=\"pil\",\n",
        "                label=\"Upload Image\"\n",
        "            )\n",
        "\n",
        "            question_input = gr.Textbox(\n",
        "                label=\"Ask a Question about the Image\",\n",
        "                placeholder=\"What is happening in the image?\"\n",
        "            )\n",
        "\n",
        "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
        "            clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "        # ---------- RIGHT COLUMN ----------\n",
        "        with gr.Column(scale=1):\n",
        "            output_box = gr.Textbox(\n",
        "                label=\"EDA + VQA Output\",\n",
        "                lines=20,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "    # Button actions\n",
        "    submit_btn.click(\n",
        "        fn=analyze_image_with_vqa,\n",
        "        inputs=[image_input, question_input],\n",
        "        outputs=output_box\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=lambda: (\"\", None, \"\"),\n",
        "        inputs=[],\n",
        "        outputs=[question_input, image_input, output_box]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "pCe07zXV6dFy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}